{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2069f04-288f-44f5-b70c-d5744d43f892",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Setup the Jupyter lab environment\n",
    "\n",
    "## Install all the required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "ef0e8fd3-bf1a-4774-b8f6-ee36586ce4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting docx2txt\n",
      "  Using cached docx2txt-0.8.tar.gz (2.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: docx2txt\n",
      "  Building wheel for docx2txt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docx2txt: filename=docx2txt-0.8-py3-none-any.whl size=3959 sha256=73d6bb0df58665a1a3aa7f050d87170a3879c6e544fa2bf2f61632d5aa4a3889\n",
      "  Stored in directory: /Users/skadthan/Library/Caches/pip/wheels/0f/0e/7a/3094a4ceefe657bff7e12dd9592a9d5b6487ef4338ace0afa6\n",
      "Successfully built docx2txt\n",
      "Installing collected packages: docx2txt\n",
      "Successfully installed docx2txt-0.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install -U --no-cache-dir boto3 pypdf langchain-community langchain sqlalchemy\n",
    "#%pip install -U --no-cache-dir langchain faiss-cpu\n",
    "#%pip install -U --no-cache-dir pinecone-client tiktoken ipywidgets matplotlib anthropic langchain-aws\n",
    "#%pip install -U --no-cache-dir transformers\n",
    "#%pip install python-pptx\n",
    "#%pip install openpyxl\n",
    "#%pip install docx2txt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620edbf4-fcc6-4f67-9c7e-de254e83d59c",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "⚠️ ⚠️ ⚠️ Before running this notebook, ensure you've run the [Bedrock boto3 setup notebook](../rag_poc/bedrock_basics.ipynb) notebook. ⚠️ ⚠️ ⚠️ Then run these installs below\n",
    "\n",
    "⚠️ ⚠️ ⚠️  Please un comment and run the pip installs if you have not done these already ⚠️ ⚠️ ⚠️ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "bb696c49-d9e2-4ac1-ace6-89f2aac9a858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from io import StringIO\n",
    "import sys\n",
    "import textwrap\n",
    "import os\n",
    "from typing import Optional\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def print_ww(*args, width: int = 100, **kwargs):\n",
    "    \"\"\"Like print(), but wraps output to `width` characters (default 100)\"\"\"\n",
    "    buffer = StringIO()\n",
    "    try:\n",
    "        _stdout = sys.stdout\n",
    "        sys.stdout = buffer\n",
    "        print(*args, **kwargs)\n",
    "        output = buffer.getvalue()\n",
    "    finally:\n",
    "        sys.stdout = _stdout\n",
    "    for line in output.splitlines():\n",
    "        print(\"\\n\".join(textwrap.wrap(line, width=width)))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "def get_bedrock_client(\n",
    "    assumed_role: Optional[str] = None,\n",
    "    region: Optional[str] = None,\n",
    "    runtime: Optional[bool] = True,\n",
    "):\n",
    "    \"\"\"Create a boto3 client for Amazon Bedrock, with optional configuration overrides\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    assumed_role :\n",
    "        Optional ARN of an AWS IAM role to assume for calling the Bedrock service. If not\n",
    "        specified, the current active credentials will be used.\n",
    "    region :\n",
    "        Optional name of the AWS Region in which the service should be called (e.g. \"us-east-1\").\n",
    "        If not specified, AWS_REGION or AWS_DEFAULT_REGION environment variable will be used.\n",
    "    runtime :\n",
    "        Optional choice of getting different client to perform operations with the Amazon Bedrock service.\n",
    "    \"\"\"\n",
    "    if region is None:\n",
    "        target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "    else:\n",
    "        target_region = region\n",
    "\n",
    "    print(f\"Create new client\\n  Using region: {target_region}\")\n",
    "    session_kwargs = {\"region_name\": target_region}\n",
    "    client_kwargs = {**session_kwargs}\n",
    "\n",
    "    profile_name = os.environ.get(\"AWS_PROFILE\")\n",
    "    if profile_name:\n",
    "        print(f\"  Using profile: {profile_name}\")\n",
    "        session_kwargs[\"profile_name\"] = profile_name\n",
    "\n",
    "    retry_config = Config(\n",
    "        region_name=target_region,\n",
    "        retries={\n",
    "            \"max_attempts\": 10,\n",
    "            \"mode\": \"standard\",\n",
    "        },\n",
    "    )\n",
    "    session = boto3.Session(**session_kwargs)\n",
    "\n",
    "    if assumed_role:\n",
    "        print(f\"  Using role: {assumed_role}\", end='')\n",
    "        sts = session.client(\"sts\")\n",
    "        response = sts.assume_role(\n",
    "            RoleArn=str(assumed_role),\n",
    "            RoleSessionName=\"langchain-llm-1\"\n",
    "        )\n",
    "        print(\" ... successful!\")\n",
    "        client_kwargs[\"aws_access_key_id\"] = response[\"Credentials\"][\"AccessKeyId\"]\n",
    "        client_kwargs[\"aws_secret_access_key\"] = response[\"Credentials\"][\"SecretAccessKey\"]\n",
    "        client_kwargs[\"aws_session_token\"] = response[\"Credentials\"][\"SessionToken\"]\n",
    "\n",
    "    if runtime:\n",
    "        service_name='bedrock-runtime'\n",
    "    else:\n",
    "        service_name='bedrock'\n",
    "\n",
    "    bedrock_client = session.client(\n",
    "        service_name=service_name,\n",
    "        config=retry_config,\n",
    "        **client_kwargs\n",
    "    )\n",
    "\n",
    "    print(\"boto3 Bedrock client successfully created!\")\n",
    "    print(bedrock_client._endpoint)\n",
    "    return bedrock_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e3805a57-3e5f-44f2-80c8-57a58774f712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: us-east-1\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock-runtime(https://bedrock-runtime.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import boto3\n",
    "\n",
    "\n",
    "# ---- ⚠️ Un-comment and edit the below lines as needed for your AWS setup ⚠️ ----\n",
    "\n",
    "# os.environ[\"AWS_DEFAULT_REGION\"] = \"<REGION_NAME>\"  # E.g. \"us-east-1\"\n",
    "# os.environ[\"AWS_PROFILE\"] = \"<YOUR_PROFILE>\"\n",
    "# os.environ[\"BEDROCK_ASSUME_ROLE\"] = \"<YOUR_ROLE_ARN>\"  # E.g. \"arn:aws:...\"\n",
    "\n",
    "\n",
    "boto3_bedrock = get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region='us-east-1' #os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef45340-de56-4226-bb61-9386e6ad16a3",
   "metadata": {},
   "source": [
    "#### FAISS as VectorStore\n",
    "\n",
    "In order to be able to use embeddings for search, we need a store that can efficiently perform vector similarity searches. In this notebook we use FAISS, which is an in memory store. For permanently store vectors, one can use pgVector, Pinecone or Chroma.\n",
    "\n",
    "The langchain VectorStore API's are available [here](https://python.langchain.com/en/harrison-docs-refactor-3-24/reference/modules/vectorstore.html)\n",
    "\n",
    "To know more about the FAISS vector store please refer to this [document](https://arxiv.org/pdf/1702.08734.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af9f33f-61ce-4f84-9bc0-2bc9a16406f4",
   "metadata": {},
   "source": [
    "#### Titan embeddings Model\n",
    "\n",
    "Embeddings are a way to represent words, phrases or any other discrete items as vectors in a continuous vector space. This allows machine learning models to perform mathematical operations on these representations and capture semantic relationships between them.\n",
    "\n",
    "Embeddings are for example used for the RAG [document search capability](https://labelbox.com/blog/how-vector-similarity-search-works/) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "aff74714-d8d0-4ab4-b1fd-7495c85cad44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents=1\n",
      "rag_data/nimbus/Relevant_Experience_Project_1.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Blurbs_relating_to_the_EDLI.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/ORES1_OC_DevOps_Self_Assessment_OY4.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Nimbus_ISFCS_MarketResearch_Capabilities.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Nimbus_RFQ #210501_ORES_Volume I_Technical_edited.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Nimbus_OC_Self_Assessment_OY3.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Nimbus_DAS_Self_Assessment_OY3.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/ORES2_BY_CPI.docx: Word Document\n",
      "Processed 28 document chunks.\n",
      "rag_data/nimbus/Nimbus_OHI_Partnership.pptx: PowerPoint Presentation\n",
      "Number of documents=1\n",
      "rag_data/nimbus/RR_Nimbus_Value_to_SMAQ_and_CCSQ_Strategy.docx: Word Document\n",
      "Processed 5 document chunks.\n",
      "rag_data/nimbus/FDA_DIMES_Capes_Matrix_Nimbus.xlsx: Excel Spreadsheet\n",
      "Number of documents=1\n",
      "rag_data/nimbus/ORES1_DAS_Self_Assessment_OY4.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/PP_Data_Call_2_Corporate_Experience_edited.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Nimbus_RFQ_210501_ORES_Volume_I_Technical_edited.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/MCSI_SSN_Nimbus_Consulting.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/CS_Nimbus_Capabilities_Statement_for_National_Directory_of_Provider_Services.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/ORES2_CTO_Self_Assessment_BY.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Nimbus_CTO_Self_Assessment_OY3.docx: Word Document\n",
      "Processed 7 document chunks.\n",
      "rag_data/nimbus/Nimbus_Consulting_Profile_and_Capabilities_CCIIO.pptx: PowerPoint Presentation\n",
      "Number of documents=1\n",
      "rag_data/nimbus/ORES1_CTO_Self_Assessment_OY4.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/NimbusConsultingCapabilityStatementforStrategicEnterpriseandAlignment_SEA_andProgramManagement_PMSupport.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/CS_Nimbus_Services.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Nimbus_RFI_RFQ_1688613_Modern_Integration_Services.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Data_Call_4_Phase_II_Technical_Writing_Assignment_LL_1_1.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/RR_CMS_PPM_RFI_Response_Final_Draft_for_Feedback.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Nimbus_CPI_Self_Assessment_OY3.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/ORES2_OY1_CPI.docx: Word Document\n",
      "Processed 17 document chunks.\n",
      "rag_data/nimbus/Nimbus_Support_for_the_Office_of_Communications_V5.pptx: PowerPoint Presentation\n",
      "Number of documents=1\n",
      "rag_data/nimbus/RR_Nimbus_RFI_Response_to_IRS.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/ORES2_CTO_1_Self_Assessment_OY1.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/CS_Nimbus_Consulting_Capability_Statement_for_Strategic_Enterprise_and_Alignment_SEA_and_Program_Management_PM_Support.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/RR_Nimbus_RFI_Response_to_FDA_SSN_125075.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/Nimbus_Consulting_TIPSS_Volume_I_Technical_edited.docx: Word Document\n",
      "Number of documents=1\n",
      "rag_data/nimbus/ORES1_CPI_Self_Assessment_OY4.docx: Word Document\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.vectorstores import FAISS\n",
    "from pathlib import Path\n",
    "\n",
    "#from langchain.embeddings import BedrockEmbeddings #Warning message, this is updated to the the following import.\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pptx import Presentation\n",
    "from openpyxl import load_workbook\n",
    "from langchain.schema import Document\n",
    "\n",
    "br_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\", client=boto3_bedrock)\n",
    "\n",
    "s3_path = \"s3://ashu-nimbus-data\"\n",
    "working_path = \"./rag_data/nimbus\"\n",
    "!aws s3 sync $s3_path $working_path\n",
    "folder_path = Path(working_path)\n",
    "vectorstore_faiss_aws = None\n",
    "\n",
    "def load_docx(file):\n",
    "    loader = Docx2txtLoader(file)\n",
    "    documents= loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=200,)\n",
    "    split_data = text_splitter.split_documents(documents)\n",
    "    print(f\"Number of documents={len(documents)}\")\n",
    "    return split_data\n",
    "\n",
    "def load_pdf(file):\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=200,)\n",
    "    split_data = text_splitter.split_documents(documents)\n",
    "    print(f\"Number of documents={len(documents)}\")\n",
    "    return split_data\n",
    "\n",
    "def load_csv(file):\n",
    "    loader = CSVLoader(file_path) # --- > 219 docs with 400 chars, each row consists in a question column and an answer column\n",
    "    documents = loader.load() #\n",
    "    print(f\"Number of documents={len(documents)}\")\n",
    "    docs = CharacterTextSplitter(chunk_size=2000, chunk_overlap=400, separator=\",\").split_documents(documents)\n",
    "    return split_data\n",
    "\n",
    "def load_xlsx(file):\n",
    "    workbook = load_workbook(file)\n",
    "    full_text = \"\"\n",
    "    documents = []\n",
    "    for sheet_name in workbook.sheetnames:\n",
    "        sheet = workbook[sheet_name]\n",
    "        for row in sheet.iter_rows(values_only=True):\n",
    "            row_text = \" \".join([str(cell) if cell is not None else \"\" for cell in row])\n",
    "            full_text += row_text + \"\\n\"\n",
    "    \n",
    "    documents.append(Document(page_content=full_text, metadata={\"source\": str(file)}))\n",
    "    # Split the text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=200,)\n",
    "    split_data = text_splitter.split_documents(documents)\n",
    "    print(f\"Processed {len(docs)} document chunks.\")\n",
    "    return split_data\n",
    "\n",
    "def load_pptx(file):\n",
    "    presentation = Presentation(file)\n",
    "    full_text = \"\"\n",
    "    documents = []\n",
    "    \n",
    "    for slide in presentation.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if shape.has_text_frame:\n",
    "                full_text += shape.text + \"\\n\"\n",
    "    \n",
    "    documents.append(Document(page_content=full_text, metadata={\"source\": str(file)}))\n",
    "    # Split the text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000,chunk_overlap=200,)\n",
    "    split_data = text_splitter.split_documents(documents)\n",
    "    print(f\"Processed {len(docs)} document chunks.\")\n",
    "    return split_data\n",
    "    \n",
    "\n",
    "for file_path in folder_path.iterdir():\n",
    "    try:\n",
    "        if file_path.is_file():\n",
    "            #print(f\"Processing file: {file_path}\")\n",
    "            file_extension = os.path.splitext(file_path)[1].lower()\n",
    "            # Check the type based on extension\n",
    "            \n",
    "            if file_extension == '.docx':\n",
    "                file_type = 'Word Document'\n",
    "                docs=load_docx(file_path)\n",
    "            elif file_extension == '.ppt' or file_extension == '.pptx':\n",
    "                file_type = 'PowerPoint Presentation'\n",
    "                docs=load_pptx(file_path)\n",
    "            elif file_extension == '.xlsx':\n",
    "                file_type = 'Excel Spreadsheet'\n",
    "                docs=load_xlsx(file_path)\n",
    "            elif file_extension == '.pdf':\n",
    "                file_type = 'PDF Document'\n",
    "                loader = PyPDFDirectoryLoader(file_path)\n",
    "                docs=load_pdf(file_path)\n",
    "            elif file_extension == '.csv':\n",
    "                file_type = 'CSV File'\n",
    "                docs=load_csv(file_path)\n",
    "            else:\n",
    "                file_type = 'Unknown Type'\n",
    "\n",
    "            print(f\"{file_path}: {file_type}\")\n",
    "\n",
    "            # Update or create the FAISS vectorstore\n",
    "            if vectorstore_faiss_aws is None:\n",
    "                vectorstore_faiss_aws = FAISS.from_documents(documents=docs,embedding = br_embeddings)\n",
    "            else:\n",
    "                vectorstore_faiss_aws.add_documents(documents=docs)     \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d14763-bb2b-4ce3-833c-8e0e8c59fe19",
   "metadata": {},
   "source": [
    "#### Auto add the history to the Chat with Retriever\n",
    "\n",
    "Wrap with Runnable Chat History with Session id and run the chat conversation\n",
    "\n",
    "borrowed from https://github.com/langchain-ai/langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "edeb1769-4766-4d19-b349-6f843c010cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<langchain_community.vectorstores.faiss.FAISS object at 0x1670a1c50>\n"
     ]
    }
   ],
   "source": [
    "print(vectorstore_faiss_aws)\n",
    "# Save the updated FAISS vectorstore if needed\n",
    "vectorstore_faiss_aws.save_local(\"./faiss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d34b415-888e-42fb-9d81-75b251be92d2",
   "metadata": {},
   "source": [
    "## Steps to read the vector DB content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30982e1b-c700-4f3a-8a30-eb7406ef5dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a query\n",
    "query = \"What is the purpose of this document?\"\n",
    "\n",
    "# Perform a similarity search\n",
    "similar_docs = vectorstore_faiss_aws.similarity_search(query, k=5)  # Retrieve top 5 similar documents\n",
    "\n",
    "# Display the retrieved documents\n",
    "for i, doc in enumerate(similar_docs):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Page Content: {doc.page_content}\")\n",
    "    print(f\"Metadata: {doc.metadata}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "e9e15942-a22c-4e0b-9941-a3e34470ef78",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "\n",
    "model_parameter = {\"temperature\": 0.0, \"top_p\": .5, \"max_tokens_to_sample\": 2000}\n",
    "modelId = \"anthropic.claude-3-sonnet-20240229-v1:0\" #\"anthropic.claude-v2\"\n",
    "chatbedrock_llm = ChatBedrock(\n",
    "    model_id=modelId,\n",
    "    client=boto3_bedrock,\n",
    "    model_kwargs=model_parameter, \n",
    "    beta_use_converse_api=True\n",
    ")\n",
    "\n",
    "    \n",
    "contextualized_question_system_template = (\n",
    "    \"You are assisting in generating a comprehensive capabilities statement based on user queries and contextual information.\"\n",
    "    \"Given the chat history and the latest user query, rewrite the query to be a self-contained question,\"\n",
    "    \"including any missing information required to answer it effectively. Ensure the question is framed professionally\"\n",
    "    \"and aligns with the goal of constructing a detailed capabilities statement.\"\n",
    "    \"Do NOT answer the question—only reformulate it. If no reformulation is needed, return the query as is.\"\n",
    ")\n",
    "\n",
    "contextualized_question_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualized_question_system_template),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    chatbedrock_llm, vectorstore_faiss_aws.as_retriever(), contextualized_question_prompt\n",
    ")\n",
    "\n",
    "\n",
    "qa_system_prompt = \"\"\"You are an AI assistant tasked with providing detailed and relevant responses for generating a capabilities statement. \\\n",
    "Using the retrieved documents and context, answer the user query as accurately and professionally as possible. \\\n",
    "If the retrieved context does not contain sufficient information to answer the query, state: \\\n",
    "\"I do not have enough context to provide an answer.\"\\\n",
    "\n",
    "Guidelines:\\\n",
    "1. Provide answers that are factual and directly relevant to constructing a capabilities statement.\\\n",
    "2. Use concise, professional language tailored to the capabilities domain.\\\n",
    "3. Reference retrieved context explicitly when possible, such as contracts, proposals, or assessments.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", qa_system_prompt),\n",
    "    MessagesPlaceholder(\"chat_history\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "question_answer_chain = create_stuff_documents_chain(chatbedrock_llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
    "\n",
    "#- Wrap the rag_chain with RunnableWithMessageHistory to automatically handle chat history:\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    #print(session_id)\n",
    "    if session_id not in store:\n",
    "        store[session_id] = InMemoryChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "6718cdb8-4285-4bbc-bae7-85d9f64591a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What kind of bias can SageMaker detect?',\n",
       " 'chat_history': [],\n",
       " 'context': [Document(metadata={'source': 'rag_data/nimbus/Nimbus_CPI_Self_Assessment_OY3.docx'}, page_content='PEOG-DSEO CMS.gov Program Integrity Knowledge and Resource Center: in March of 2020, the Nimbus Web Developers designed, built, and edited content for this new CPI web page. The goal was to have a page to serve as a training resource for medical reviewers, program integrity and appeals contractors, CMS staff, state Medicaid agency staff, and other stakeholders.\\n\\nBenefits to CMS: This page was necessary to resolve a failure on IBM’s part concerning the Extrapolation and Estimation eLearning course; IBM spent approximately one year developing the course, which was unusable when it was time to launch due to the burden on test-takers to submit to an Experian identity check in order to proceed to the course questions. DSEO’s management agreed the Experian questions were unreasonable and too probing for an eLearning module, so we worked with Fors Marsh Group and OC who obtained the HTML version of the course from IBM and moved to CMS.gov. Allowing users to access the course from the new CPI web page enhances user experience and trust. The new web page was launched as soon as the course was uploaded and tested, allowing users to take the course on schedule.'),\n",
       "  Document(metadata={'source': 'rag_data/nimbus/Nimbus_Support_for_the_Office_of_Communications_V5.pptx'}, page_content='New Features Delivered to Production\\nFewer Issues in the Backlog\\nSprint Capacity\\nReleases over Time\\nEmpowered Business and Operations\\nBusiness owners can monitor site utilization and effectiveness\\nOperations staff can monitor utilization and load\\nNo need to log into multiple tools\\nNo need to provide and maintain admin access to multiple tools for multiple users \\n2/9/2021\\n6\\nWe simplified operations and monitoring by providing a single-pane-of-glass\\nActive Users and Number of Requests\\nWhy We Are Unique\\n\\nNimbus has invested in a mentorship program with Gartner to improve our ability to provide value and improve outcomes for our clients. \\nNimbus Values Research with Objectivity – we have no vendor or tools bias, we evaluate the needs of the organization and provide the “best fit” \\nDASG has embarked on a transformation; moving to agile and moving to cloud\\nDASG’s needs and objectives will be different from OC\\nIn-depth CMS IT program experience and understanding of CMS culture\\nNimbus can work with multiple teams to break down silos and find common needs\\nNimbus can choose the right tool for the job\\nKnowledge of TRA Standards and VDC/cloud technology and the ability to respond quickly helps CMS projects avoid delays in conforming to CMS technical standards\\nNo conflicts of interest; not a Prime on Sparc\\n02/09/2021\\n7\\nHow can we help? \\nOpen Discussion\\n2/9/2021\\n8\\nResearch backed by Gartner\\n2/9/2021\\n9\\nWe simplified operations and monitoring by providing a single-pane-of-glass\\nGraphic—Dashboard, APIs, icons for all the new tools, simplified access management (don’t have to maintain admin access to multiple tools) \\nNotes\\n2/9/2021\\n10'),\n",
       "  Document(metadata={'source': 'rag_data/nimbus/ORES1_CPI_Self_Assessment_OY4.docx'}, page_content='Nimbus staff work behind the scenes to stay in touch with each other, and share expertise across organizational lines.  Here are a few examples of that Cross-Team collaboration: \\n\\nIn September 2020, the Nimbus Business and Data Analytics Resource, Zaheer Ginna, used his SAS expertise to address an issue with performing analytics on PECOS 2.0 data.  SAS’ inability to read the JSON data stored in Postgres SQL in a single column was causing long-running queries and blocking data analytics on PECOS 2.0. Zaheer worked with SAS to resolve the issue and open the door to expand the analytics by accessing Postgres complex functionalities such as arrays and accessing hierarchical data.  Benefit to CMS: Advanced analytics are the lifeblood of the CMS mission. Data transformed into information drives policy decisions and congressional mandates that in turn impact our ability to deliver quality healthcare. The PECOS 2.0 redesign will fully modernize Provider Enrollments in the Cloud. SAS limitations had the potential to impact mission critical analytics to the new system leading to concerns that SAS may need to be replaced and/or PECOS 2.0 design changes made. The solution avoided schedule and cost risks associated with the potential replacement of SAS. It also ensured that PECOS 2.0 need not be delayed to retool, and that analytics against the final solution will satisfy the CMS mission.'),\n",
       "  Document(metadata={'source': 'rag_data/nimbus/Data_Call_4_Phase_II_Technical_Writing_Assignment_LL_1_1.docx'}, page_content='Chance of occurrence is unlikely (Point = 2)\\n\\nChance of occurrence is moderate (Point = 3)\\n\\nChance of occurrence is likely (Point = 4)\\n\\nChance of occurrence is highly likely (Point = 5)\\n\\n\\n\\nThe next step is to identify the potential consequences of a risk. Factors that are considered in the evaluation of consequences include data loss, system or application downtime, monetary loss, and legal consequences. We recommend assessing the consequence using the following five-point scale:\\n\\nNo impact (Point = 1)\\n\\nAcceptable (Point = 2)\\n\\nTolerable (Point = 3)\\n\\nUndesirable (Point = 4)\\n\\nIntolerable (Point = 5)\\n\\n\\n\\nThe risk rating is the product of multiplying the probability and the consequence, which, therefore, ranges from 1 to 25. A risk rating between 15 and 25 is considered “high” and should have a documented mitigation strategy. High risks and their mitigation strategies must be reviewed more frequently than other risks.\\n\\n\\n\\nA risk that manifests is then an issue and is tracked using the issue management processes. \\n\\n\\n\\nA robust risk management process needs to be supported by a tool that is easy to use and can be configured to support the processes. We will collaborate with the incumbent on transitioning the current tool used for risk management and solicit feedback from MITG on how they use the tool and associated challenges. We will thereafter recommend changes to the current tool, or alternatives that may be better suited. \\n\\n\\n\\nA tool used for Risk Management should be able to support the following:\\n\\nCapture attributes of risk such as definition, trigger, probability, consequence, risk rating, mitigation strategy, and others\\n\\nSet a cadence for review of risks based on risk ratings. For example, high risks to be reviewed every 30 days, while moderate every 60 days, and others every 90 days\\n\\nAbility to maintain an audit of changes to the risk \\n\\nConvert a risk to an issue and trigger the issue management workflow')],\n",
       " 'answer': \"I'm afraid I don't have enough context about SageMaker's specific capabilities to provide details on what types of bias it can detect. SageMaker is Amazon's machine learning platform, but without more specifics on which SageMaker service or feature you are asking about, I cannot give an accurate answer regarding bias detection. Could you provide some more context about the particular SageMaker component you have in mind? That would help me better understand and respond to your query.\"}"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chain_with_history.invoke(\n",
    "    {\"input\": \"What kind of bias can SageMaker detect?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdd0d61-b1a8-41d0-8ff6-797d6120d9ce",
   "metadata": {},
   "source": [
    "### As a follow on question\n",
    "\n",
    "1. The phrase `it` will be converted based on the chat history\n",
    "2. Retriever gets invoked to get relevant content based on chat history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "aafd3eac-3f2a-41b5-b94c-4d7c732594e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Please help me write 2 pager of full capability statement for the services/tasks such as\n",
      "Project Management Support, data architecture, data quality, system architecture areas?',\n",
      "'chat_history': [HumanMessage(content='What kind of bias can SageMaker detect?',\n",
      "additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm afraid I don't have enough\n",
      "context about SageMaker's specific capabilities to provide details on what types of bias it can\n",
      "detect. SageMaker is Amazon's machine learning platform, but without more specifics on which\n",
      "SageMaker service or feature you are asking about, I cannot give an accurate answer regarding bias\n",
      "detection. Could you provide some more context about the particular SageMaker component you have in\n",
      "mind? That would help me better understand and respond to your query.\", additional_kwargs={},\n",
      "response_metadata={})], 'context': [Document(metadata={'source':\n",
      "'rag_data/nimbus/RR_CMS_PPM_RFI_Response_Final_Draft_for_Feedback.docx'}, page_content=\"We are\n",
      "pleased to provide the answers to the questions asked in the RFI. We leverage the research knowledge\n",
      "base of Gartner Inc. and other industry-leading generative AI research solutions to deliver our\n",
      "services and strategic advice to federal government agencies. Our research is based on a compilation\n",
      "and review of several industry-leading publications and is backed by Gartner. We request that CMS\n",
      "consider Nimbus in future solicitations related to these requirements.\\n\\n\\n\\nWhat additional\n",
      "information not already included the DRAFT Performance Work Statement (PWS) would you like to see\n",
      "included to help you be more successful in a bid? \\n\\nThe current Performance Work Statement (PWS)\n",
      "provides a useful overview of the tasks but lacks the depth needed to gauge the work involved\n",
      "accurately. For instance, more specific details about access to CMS stakeholders, including the\n",
      "expected frequency, type, and purpose of meetings and a clearer outline of the decision-making\n",
      "processes within the PPMS framework, would enhance our ability to propose a well-informed\n",
      "stakeholder engagement strategy.\\n\\nAdditionally, while the PWS outlines the tasks, it does not\n",
      "sufficiently specify the required experience level or the labor category mix. More granular\n",
      "information about staffing requirements, including the number of staff, their roles, and the level\n",
      "of expertise expected for each task, would allow us to craft a bid that aligns closely with CMS's\n",
      "needs. Multiple tasks reference lessons learned and best practices as inputs for future work.\n",
      "Clarifying if additional scope has been added due to these lessons and any information on challenges\n",
      "faced by previous contractors would provide valuable insights into potential risks and areas for\n",
      "improvement or innovation.\"), Document(metadata={'source':\n",
      "'rag_data/nimbus/RR_CMS_PPM_RFI_Response_Final_Draft_for_Feedback.docx'}, page_content=\"What are\n",
      "factors that you think CMS should consider when preparing a potential formal requirement?\\n\\nCMS\n",
      "should ensure alignment with OIT's strategic goals when preparing formal requirements. A\n",
      "comprehensive needs assessment involving OIT and other Centers/office stakeholders that PPMS will\n",
      "support will help CMS identify actual & current needs rather than perceived ones that may not be\n",
      "relevant anymore. CMS should gather input from key stakeholders, including PPMS program managers,\n",
      "its end-users, and external partners, to identify pain points and desired outcomes from the PPMS\n",
      "program. Clarity and specificity are crucial; requirements should include defined deliverables,\n",
      "objectives, and acceptance criteria to avoid misunderstandings. \\n\\n\\n\\nCMS should ensure that the\n",
      "PWS requirement allows for scalability to accommodate future and flexibility to adapt to evolving\n",
      "needs. CMS should establish specific performance metrics (KPIs) to evaluate success and ensure\n",
      "accountability. CMS should require a risk management plan as part of the requirement. This plan\n",
      "should outline how risks will be identified, tracked, mitigated, and managed throughout the project\n",
      "lifecycle.\\n\\n\\n\\nTechnical support and knowledge transfer provisions will ensure continuity. Cost-\n",
      "benefit analysis and budget constraints should guide CMS in prioritizing PPMS task areas that offer\n",
      "value. \\n\\n\\n\\nEncouraging consultative approaches to adopting innovative technologies like AI and\n",
      "machine learning can drive modernization efforts at CMS.\\n\\n\\n\\nHow likely are you to submit a\n",
      "response when this RFQ is released? Would you be submitting as a prime? \\n\\nNimbus has the\n",
      "capabilities, resources, and past performance to meet the requirements as a prime contractor, and\n",
      "leading this project aligns with our strategic objectives. We will likely submit as a prime\n",
      "contractor, depending on the clarifications on the final RFP's scope, clear requirements, and\n",
      "assessment of any organizational conflicts of interest.\\n\\n6\\n\\n\\n\\n1\"),\n",
      "Document(metadata={'source': 'rag_data/nimbus/NimbusConsultingCapabilityStatementforStrategicEnterpr\n",
      "iseandAlignment_SEA_andProgramManagement_PMSupport.docx'}, page_content='Division of Investigative\n",
      "Systems Management (DISM) (CPI) – We developed DISM Portfolio management that created new\n",
      "capabilities in Jira to link program-level epics with portfolio initiatives and build new\n",
      "dashboards. A consolidated view showing accomplishments across multiple systems gives leadership and\n",
      "others insight into team accomplishments to communicate successes and generate synergy to move\n",
      "forward together to achieve DISM objectives and support the CPI mission. Portfolio management\n",
      "objectives are all linked to the Division OKRs. \\n\\nUCM (CPI) - \\tRefer to our Stakeholder\n",
      "Assessment initiative listed above under 4.b.\\n\\n5. Experience with Procurement Spend Optimization.\n",
      "\\n\\nWe have not had the opportunity yet to work with OAGM, thus we do not have this experience.\n",
      "However, we plan to augment our capabilities by partnering with another small business with\n",
      "experience in these areas. \\n\\n6. Experience in evaluating data regarding Equitable Recruiting.\n",
      "\\n\\nWe have not had the opportunity yet to work with OAGM, thus we do not have this experience.\n",
      "However, we plan to augment our capabilities by partnering with another small business with\n",
      "experience in these areas. \\n\\n7. Experience with reducing Unliquidated Obligations (ULOs). \\n\\nWe\n",
      "have not had the opportunity yet to work with OAGM, thus we do not have this experience. However, we\n",
      "plan to augment our capabilities by partnering with another small business with experience in these\n",
      "areas.\\n\\n8. Experience with Customer Experience (CX), including support to both formulate and\n",
      "maintain a CX strategy for all new projects and professional services to customize each CX strategy\n",
      "to the needs of the project and their users, with the goal of moving from identifying a problem to\n",
      "presenting a customer-focused solution(s). \\n\\nWe have some experience with Customer Experience in a\n",
      "supporting role working with other teams. However, we plan to augment our capabilities by partnering\n",
      "with another small business with experience in this area.'), Document(metadata={'source': 'rag_data/\n",
      "nimbus/CS_Nimbus_Consulting_Capability_Statement_for_Strategic_Enterprise_and_Alignment_SEA_and_Prog\n",
      "ram_Management_PM_Support.docx'}, page_content='Division of Investigative Systems Management (DISM)\n",
      "(CPI) – We developed DISM Portfolio management that created new capabilities in Jira to link\n",
      "program-level epics with portfolio initiatives and build new dashboards. A consolidated view showing\n",
      "accomplishments across multiple systems gives leadership and others insight into team\n",
      "accomplishments to communicate successes and generate synergy to move forward together to achieve\n",
      "DISM objectives and support the CPI mission. Portfolio management objectives are all linked to the\n",
      "Division OKRs. \\n\\nUCM (CPI) - \\tRefer to our Stakeholder Assessment initiative listed above under\n",
      "4.b.\\n\\n5. Experience with Procurement Spend Optimization. \\n\\nWe have not had the opportunity yet\n",
      "to work with OAGM, thus we do not have this experience. However, we plan to augment our capabilities\n",
      "by partnering with another small business with experience in these areas. \\n\\n6. Experience in\n",
      "evaluating data regarding Equitable Recruiting. \\n\\nWe have not had the opportunity yet to work with\n",
      "OAGM, thus we do not have this experience. However, we plan to augment our capabilities by\n",
      "partnering with another small business with experience in these areas. \\n\\n7. Experience with\n",
      "reducing Unliquidated Obligations (ULOs). \\n\\nWe have not had the opportunity yet to work with OAGM,\n",
      "thus we do not have this experience. However, we plan to augment our capabilities by partnering with\n",
      "another small business with experience in these areas.\\n\\n8. Experience with Customer Experience\n",
      "(CX), including support to both formulate and maintain a CX strategy for all new projects and\n",
      "professional services to customize each CX strategy to the needs of the project and their users,\n",
      "with the goal of moving from identifying a problem to presenting a customer-focused solution(s).\n",
      "\\n\\nWe have some experience with Customer Experience in a supporting role working with other teams.\n",
      "However, we plan to augment our capabilities by partnering with another small business with\n",
      "experience in this area.')], 'answer': \"Here is a draft 2-page capability statement highlighting our\n",
      "services and experience in project management support, data architecture, data quality, and system\n",
      "architecture:\\n\\nCapability Statement\\n\\nCompany Overview\\n[Company Name] is a leading provider of\n",
      "comprehensive information technology and data management solutions to federal agencies. With over\n",
      "[XX] years of experience, we specialize in delivering expert project management, data architecture,\n",
      "data quality, and system architecture services to drive successful outcomes for our clients' most\n",
      "critical initiatives.  \\n\\nCore Capabilities\\n\\nProject Management Support\\n• Proven full lifecycle\n",
      "project/program management following industry best practices (PMI, Agile, etc.)\\n• Experienced\n",
      "certified Project Management Professionals (PMPs) and Scrum Masters\\n• Expertise in developing\n",
      "project plans, schedules, risk/issue tracking, status reporting\\n• Skilled at leading diverse\n",
      "project teams and managing stakeholder communications\\n• Proficient in project management tools like\n",
      "MS Project, Jira, Confluence\\n\\nData Architecture \\n• Designing and implementing enterprise data\n",
      "models, schemas, and data warehouses\\n• Defining data standards, governance policies, and data\n",
      "integration strategies  \\n• Experienced with dimensional modeling, ETL processes, and BI/reporting\n",
      "solutions\\n• Skilled in data architecture/modeling tools like ERwin, PowerDesigner, Oracle, SQL\n",
      "Server\\n• Knowledge of data architecture frameworks like TOGAF, Zachman, etc.\\n\\nData Quality\\n•\n",
      "Developing data quality management plans and processes \\n• Performing data profiling, cleansing,\n",
      "deduplication, and standardization\\n• Implementing data quality rules, validation checks, and\n",
      "monitoring\\n• Experience with data quality tools like Informatica, SAS, Talend, etc.\\n• Expertise in\n",
      "data quality dimensions - accuracy, completeness, consistency, etc.\\n\\nSystem Architecture\\n•\n",
      "Designing secure, scalable, and high-performance system architectures \\n• Defining architecture\n",
      "standards, patterns, components, and deployment models\\n• Experienced with SOA, microservices, APIs,\n",
      "cloud/hybrid architectures  \\n• Proficient in architecture modeling using UML, BPMN, ArchiMate,\n",
      "etc.\\n• Knowledge of architecture frameworks like TOGAF, Zachman, FEA, etc.\\n\\nPast Performance\n",
      "Highlights\\n\\n[Project 1 Example]\\n• Provided full project management and system architecture\n",
      "support for XYZ agency's new data warehouse initiative\\n• Defined the technical architecture and led\n",
      "the implementation using AWS cloud\\n• Established data governance processes and performed data\n",
      "quality remediation  \\n\\n[Project 2 Example]  \\n• Delivered data architecture and modeling for PQR\n",
      "agency's enterprise data integration program\\n• Designed the target data models, metadata\n",
      "repository, and ETL processes\\n• Implemented data quality rules and monitoring for over 200 data\n",
      "sources\\n\\nOur team of certified professionals has an outstanding track record of successfully\n",
      "leading complex IT projects from inception through deployment. We combine technical expertise with a\n",
      "deep understanding of federal requirements to provide comprehensive, value-driven solutions tailored\n",
      "to our clients' unique needs.\"}\n"
     ]
    }
   ],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Please help me write 2 pager of full capability statement for the services/tasks such as Project Management Support, data architecture, data quality, system architecture areas?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "9b1f8e16-82c9-42b8-acac-5579201c3d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Instead of providing example projects (such as project 1, project 2 etc.,), could you\n",
      "please me fetch the actual project names that was used to pulled the data for past performances?',\n",
      "'chat_history': [HumanMessage(content='What kind of bias can SageMaker detect?',\n",
      "additional_kwargs={}, response_metadata={}), AIMessage(content=\"I'm afraid I don't have enough\n",
      "context about SageMaker's specific capabilities to provide details on what types of bias it can\n",
      "detect. SageMaker is Amazon's machine learning platform, but without more specifics on which\n",
      "SageMaker service or feature you are asking about, I cannot give an accurate answer regarding bias\n",
      "detection. Could you provide some more context about the particular SageMaker component you have in\n",
      "mind? That would help me better understand and respond to your query.\", additional_kwargs={},\n",
      "response_metadata={}), HumanMessage(content='Please help me write 2 pager of full capability\n",
      "statement for the services/tasks such as Project Management Support, data architecture, data\n",
      "quality, system architecture areas?', additional_kwargs={}, response_metadata={}),\n",
      "AIMessage(content=\"Here is a draft 2-page capability statement highlighting our services and\n",
      "experience in project management support, data architecture, data quality, and system\n",
      "architecture:\\n\\nCapability Statement\\n\\nCompany Overview\\n[Company Name] is a leading provider of\n",
      "comprehensive information technology and data management solutions to federal agencies. With over\n",
      "[XX] years of experience, we specialize in delivering expert project management, data architecture,\n",
      "data quality, and system architecture services to drive successful outcomes for our clients' most\n",
      "critical initiatives.  \\n\\nCore Capabilities\\n\\nProject Management Support\\n• Proven full lifecycle\n",
      "project/program management following industry best practices (PMI, Agile, etc.)\\n• Experienced\n",
      "certified Project Management Professionals (PMPs) and Scrum Masters\\n• Expertise in developing\n",
      "project plans, schedules, risk/issue tracking, status reporting\\n• Skilled at leading diverse\n",
      "project teams and managing stakeholder communications\\n• Proficient in project management tools like\n",
      "MS Project, Jira, Confluence\\n\\nData Architecture \\n• Designing and implementing enterprise data\n",
      "models, schemas, and data warehouses\\n• Defining data standards, governance policies, and data\n",
      "integration strategies  \\n• Experienced with dimensional modeling, ETL processes, and BI/reporting\n",
      "solutions\\n• Skilled in data architecture/modeling tools like ERwin, PowerDesigner, Oracle, SQL\n",
      "Server\\n• Knowledge of data architecture frameworks like TOGAF, Zachman, etc.\\n\\nData Quality\\n•\n",
      "Developing data quality management plans and processes \\n• Performing data profiling, cleansing,\n",
      "deduplication, and standardization\\n• Implementing data quality rules, validation checks, and\n",
      "monitoring\\n• Experience with data quality tools like Informatica, SAS, Talend, etc.\\n• Expertise in\n",
      "data quality dimensions - accuracy, completeness, consistency, etc.\\n\\nSystem Architecture\\n•\n",
      "Designing secure, scalable, and high-performance system architectures \\n• Defining architecture\n",
      "standards, patterns, components, and deployment models\\n• Experienced with SOA, microservices, APIs,\n",
      "cloud/hybrid architectures  \\n• Proficient in architecture modeling using UML, BPMN, ArchiMate,\n",
      "etc.\\n• Knowledge of architecture frameworks like TOGAF, Zachman, FEA, etc.\\n\\nPast Performance\n",
      "Highlights\\n\\n[Project 1 Example]\\n• Provided full project management and system architecture\n",
      "support for XYZ agency's new data warehouse initiative\\n• Defined the technical architecture and led\n",
      "the implementation using AWS cloud\\n• Established data governance processes and performed data\n",
      "quality remediation  \\n\\n[Project 2 Example]  \\n• Delivered data architecture and modeling for PQR\n",
      "agency's enterprise data integration program\\n• Designed the target data models, metadata\n",
      "repository, and ETL processes\\n• Implemented data quality rules and monitoring for over 200 data\n",
      "sources\\n\\nOur team of certified professionals has an outstanding track record of successfully\n",
      "leading complex IT projects from inception through deployment. We combine technical expertise with a\n",
      "deep understanding of federal requirements to provide comprehensive, value-driven solutions tailored\n",
      "to our clients' unique needs.\", additional_kwargs={}, response_metadata={})], 'context':\n",
      "[Document(metadata={'source': 'rag_data/nimbus/NimbusConsultingCapabilityStatementforStrategicEnterp\n",
      "riseandAlignment_SEA_andProgramManagement_PMSupport.docx'}, page_content='Division of Investigative\n",
      "Systems Management (DISM) (CPI) – We developed DISM Portfolio management that created new\n",
      "capabilities in Jira to link program-level epics with portfolio initiatives and build new\n",
      "dashboards. A consolidated view showing accomplishments across multiple systems gives leadership and\n",
      "others insight into team accomplishments to communicate successes and generate synergy to move\n",
      "forward together to achieve DISM objectives and support the CPI mission. Portfolio management\n",
      "objectives are all linked to the Division OKRs. \\n\\nUCM (CPI) - \\tRefer to our Stakeholder\n",
      "Assessment initiative listed above under 4.b.\\n\\n5. Experience with Procurement Spend Optimization.\n",
      "\\n\\nWe have not had the opportunity yet to work with OAGM, thus we do not have this experience.\n",
      "However, we plan to augment our capabilities by partnering with another small business with\n",
      "experience in these areas. \\n\\n6. Experience in evaluating data regarding Equitable Recruiting.\n",
      "\\n\\nWe have not had the opportunity yet to work with OAGM, thus we do not have this experience.\n",
      "However, we plan to augment our capabilities by partnering with another small business with\n",
      "experience in these areas. \\n\\n7. Experience with reducing Unliquidated Obligations (ULOs). \\n\\nWe\n",
      "have not had the opportunity yet to work with OAGM, thus we do not have this experience. However, we\n",
      "plan to augment our capabilities by partnering with another small business with experience in these\n",
      "areas.\\n\\n8. Experience with Customer Experience (CX), including support to both formulate and\n",
      "maintain a CX strategy for all new projects and professional services to customize each CX strategy\n",
      "to the needs of the project and their users, with the goal of moving from identifying a problem to\n",
      "presenting a customer-focused solution(s). \\n\\nWe have some experience with Customer Experience in a\n",
      "supporting role working with other teams. However, we plan to augment our capabilities by partnering\n",
      "with another small business with experience in this area.'), Document(metadata={'source': 'rag_data/\n",
      "nimbus/CS_Nimbus_Consulting_Capability_Statement_for_Strategic_Enterprise_and_Alignment_SEA_and_Prog\n",
      "ram_Management_PM_Support.docx'}, page_content='Division of Investigative Systems Management (DISM)\n",
      "(CPI) – We developed DISM Portfolio management that created new capabilities in Jira to link\n",
      "program-level epics with portfolio initiatives and build new dashboards. A consolidated view showing\n",
      "accomplishments across multiple systems gives leadership and others insight into team\n",
      "accomplishments to communicate successes and generate synergy to move forward together to achieve\n",
      "DISM objectives and support the CPI mission. Portfolio management objectives are all linked to the\n",
      "Division OKRs. \\n\\nUCM (CPI) - \\tRefer to our Stakeholder Assessment initiative listed above under\n",
      "4.b.\\n\\n5. Experience with Procurement Spend Optimization. \\n\\nWe have not had the opportunity yet\n",
      "to work with OAGM, thus we do not have this experience. However, we plan to augment our capabilities\n",
      "by partnering with another small business with experience in these areas. \\n\\n6. Experience in\n",
      "evaluating data regarding Equitable Recruiting. \\n\\nWe have not had the opportunity yet to work with\n",
      "OAGM, thus we do not have this experience. However, we plan to augment our capabilities by\n",
      "partnering with another small business with experience in these areas. \\n\\n7. Experience with\n",
      "reducing Unliquidated Obligations (ULOs). \\n\\nWe have not had the opportunity yet to work with OAGM,\n",
      "thus we do not have this experience. However, we plan to augment our capabilities by partnering with\n",
      "another small business with experience in these areas.\\n\\n8. Experience with Customer Experience\n",
      "(CX), including support to both formulate and maintain a CX strategy for all new projects and\n",
      "professional services to customize each CX strategy to the needs of the project and their users,\n",
      "with the goal of moving from identifying a problem to presenting a customer-focused solution(s).\n",
      "\\n\\nWe have some experience with Customer Experience in a supporting role working with other teams.\n",
      "However, we plan to augment our capabilities by partnering with another small business with\n",
      "experience in this area.'), Document(metadata={'source':\n",
      "'rag_data/nimbus/RR_CMS_PPM_RFI_Response_Final_Draft_for_Feedback.docx'}, page_content=\"We are\n",
      "pleased to provide the answers to the questions asked in the RFI. We leverage the research knowledge\n",
      "base of Gartner Inc. and other industry-leading generative AI research solutions to deliver our\n",
      "services and strategic advice to federal government agencies. Our research is based on a compilation\n",
      "and review of several industry-leading publications and is backed by Gartner. We request that CMS\n",
      "consider Nimbus in future solicitations related to these requirements.\\n\\n\\n\\nWhat additional\n",
      "information not already included the DRAFT Performance Work Statement (PWS) would you like to see\n",
      "included to help you be more successful in a bid? \\n\\nThe current Performance Work Statement (PWS)\n",
      "provides a useful overview of the tasks but lacks the depth needed to gauge the work involved\n",
      "accurately. For instance, more specific details about access to CMS stakeholders, including the\n",
      "expected frequency, type, and purpose of meetings and a clearer outline of the decision-making\n",
      "processes within the PPMS framework, would enhance our ability to propose a well-informed\n",
      "stakeholder engagement strategy.\\n\\nAdditionally, while the PWS outlines the tasks, it does not\n",
      "sufficiently specify the required experience level or the labor category mix. More granular\n",
      "information about staffing requirements, including the number of staff, their roles, and the level\n",
      "of expertise expected for each task, would allow us to craft a bid that aligns closely with CMS's\n",
      "needs. Multiple tasks reference lessons learned and best practices as inputs for future work.\n",
      "Clarifying if additional scope has been added due to these lessons and any information on challenges\n",
      "faced by previous contractors would provide valuable insights into potential risks and areas for\n",
      "improvement or innovation.\"), Document(metadata={'source':\n",
      "'rag_data/nimbus/RR_CMS_PPM_RFI_Response_Final_Draft_for_Feedback.docx'}, page_content=\"Are there\n",
      "assumptions that you made when reviewing the PWS that you can share with us? \\n\\nDue to limited\n",
      "information on some tasks regarding their scope and extent, determining the appropriate staffing mix\n",
      "and project oversight requirements is challenging. As a result, reasonable assumptions must be made\n",
      "about the remaining and ongoing work to propose an optimal resource mix. We request that CMS provide\n",
      "details on the current resource breakdown and the required work for each task. AI resources with\n",
      "industry experience often fall into higher labor categories than traditional IT roles. For this\n",
      "reason, it would be beneficial if the Government specifies the level of expertise needed in addition\n",
      "to detailing AI-related tasks in the PWS. This will enable us to tailor responses that align with\n",
      "CMS's expectations.\\n\\nWe have made the following assumptions:\\n\\nAll tasks listed in the PWS are to\n",
      "be fully executed regardless of current completion status.\\n\\nTechnical resources provided will\n",
      "possess 15+ years of experience.\\n\\nAll work will be performed remotely.\\n\\nNo travel is anticipated\n",
      "as part of the work scope.\\n\\nCMS will provide all necessary tools and cover licensing costs.\\n\\nThe\n",
      "awardee will be permitted to hire incumbent resources without any contractual restrictions.\\n\\nAre\n",
      "there team composition constraints that you would like to see included in a potential formal\n",
      "requirement (e.g., number of key personnel or key personnel roles, estimated team size, etc.)? Are\n",
      "there constraints that you have found limiting in past requests for proposals?\")], 'answer':\n",
      "'Unfortunately, I do not have access to actual project names or details from your company\\'s past\n",
      "performance records. As an AI assistant without direct connection to your internal data, I do not\n",
      "have a way to retrieve that proprietary information. \\n\\nWhen drafting capability statements or past\n",
      "performance sections, it is best practice to use real project names, descriptions and metrics that\n",
      "accurately represent the work your company has performed for previous clients. This provides\n",
      "concrete evidence of your relevant experience and capabilities.\\n\\nSince I do not have your\n",
      "company\\'s real past performance data, I populated those sections with hypothetical \"Project 1\" and\n",
      "\"Project 2\" examples. To make this capability statement fully accurate and compliant, you would need\n",
      "to replace those fictional examples with factual past performance info from your company\\'s records.\n",
      "This could include the actual project names, client names, dates, summary of the work performed,\n",
      "deliverables, benefits/results achieved, and any other quantifiable metrics that demonstrate your\n",
      "prior successes.\\n\\nPlease let me know if you would like me to modify or expand any other sections\n",
      "of the draft capability statement within the scope of the information I can reasonably provide as an\n",
      "AI assistant. I\\'m happy to keep refining it as much as possible.'}\n"
     ]
    }
   ],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Instead of providing example projects (such as project 1, project 2 etc.,), could you please me fetch the actual project names that was used to pulled the data for past performances?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "print_ww(follow_up_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "31142c3f-4474-49eb-a94f-5856437b8c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'The data quality section is very narrow, could you please expand on that a bit more? ',\n",
       " 'chat_history': [HumanMessage(content='What kind of bias can SageMaker detect?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"I'm afraid I don't have enough context about SageMaker's specific capabilities to provide details on what types of bias it can detect. SageMaker is Amazon's machine learning platform, but without more specifics on which SageMaker service or feature you are asking about, I cannot give an accurate answer regarding bias detection. Could you provide some more context about the particular SageMaker component you have in mind? That would help me better understand and respond to your query.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Please help me write 2 pager of full capability statement for the services/tasks such as Project Management Support, data architecture, data quality, system architecture areas?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content=\"Here is a draft 2-page capability statement highlighting our services and experience in project management support, data architecture, data quality, and system architecture:\\n\\nCapability Statement\\n\\nCompany Overview\\n[Company Name] is a leading provider of comprehensive information technology and data management solutions to federal agencies. With over [XX] years of experience, we specialize in delivering expert project management, data architecture, data quality, and system architecture services to drive successful outcomes for our clients' most critical initiatives.  \\n\\nCore Capabilities\\n\\nProject Management Support\\n• Proven full lifecycle project/program management following industry best practices (PMI, Agile, etc.)\\n• Experienced certified Project Management Professionals (PMPs) and Scrum Masters\\n• Expertise in developing project plans, schedules, risk/issue tracking, status reporting\\n• Skilled at leading diverse project teams and managing stakeholder communications\\n• Proficient in project management tools like MS Project, Jira, Confluence\\n\\nData Architecture \\n• Designing and implementing enterprise data models, schemas, and data warehouses\\n• Defining data standards, governance policies, and data integration strategies  \\n• Experienced with dimensional modeling, ETL processes, and BI/reporting solutions\\n• Skilled in data architecture/modeling tools like ERwin, PowerDesigner, Oracle, SQL Server\\n• Knowledge of data architecture frameworks like TOGAF, Zachman, etc.\\n\\nData Quality\\n• Developing data quality management plans and processes \\n• Performing data profiling, cleansing, deduplication, and standardization\\n• Implementing data quality rules, validation checks, and monitoring\\n• Experience with data quality tools like Informatica, SAS, Talend, etc.\\n• Expertise in data quality dimensions - accuracy, completeness, consistency, etc.\\n\\nSystem Architecture\\n• Designing secure, scalable, and high-performance system architectures \\n• Defining architecture standards, patterns, components, and deployment models\\n• Experienced with SOA, microservices, APIs, cloud/hybrid architectures  \\n• Proficient in architecture modeling using UML, BPMN, ArchiMate, etc.\\n• Knowledge of architecture frameworks like TOGAF, Zachman, FEA, etc.\\n\\nPast Performance Highlights\\n\\n[Project 1 Example]\\n• Provided full project management and system architecture support for XYZ agency's new data warehouse initiative\\n• Defined the technical architecture and led the implementation using AWS cloud\\n• Established data governance processes and performed data quality remediation  \\n\\n[Project 2 Example]  \\n• Delivered data architecture and modeling for PQR agency's enterprise data integration program\\n• Designed the target data models, metadata repository, and ETL processes\\n• Implemented data quality rules and monitoring for over 200 data sources\\n\\nOur team of certified professionals has an outstanding track record of successfully leading complex IT projects from inception through deployment. We combine technical expertise with a deep understanding of federal requirements to provide comprehensive, value-driven solutions tailored to our clients' unique needs.\", additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='Instead of providing example projects (such as project 1, project 2 etc.,), could you please me fetch the actual project names that was used to pulled the data for past performances?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Unfortunately, I do not have access to actual project names or details from your company\\'s past performance records. As an AI assistant without direct connection to your internal data, I do not have a way to retrieve that proprietary information. \\n\\nWhen drafting capability statements or past performance sections, it is best practice to use real project names, descriptions and metrics that accurately represent the work your company has performed for previous clients. This provides concrete evidence of your relevant experience and capabilities.\\n\\nSince I do not have your company\\'s real past performance data, I populated those sections with hypothetical \"Project 1\" and \"Project 2\" examples. To make this capability statement fully accurate and compliant, you would need to replace those fictional examples with factual past performance info from your company\\'s records. This could include the actual project names, client names, dates, summary of the work performed, deliverables, benefits/results achieved, and any other quantifiable metrics that demonstrate your prior successes.\\n\\nPlease let me know if you would like me to modify or expand any other sections of the draft capability statement within the scope of the information I can reasonably provide as an AI assistant. I\\'m happy to keep refining it as much as possible.', additional_kwargs={}, response_metadata={})],\n",
       " 'context': [Document(metadata={'source': 'rag_data/nimbus/NimbusConsultingCapabilityStatementforStrategicEnterpriseandAlignment_SEA_andProgramManagement_PMSupport.docx'}, page_content='Data Governance (OIT) - We have provided expert consultation to CMS for the planning and implementation of an Enterprise-wide Data Governance program.\\n\\nEnterprise Data Lake (EDL) (OIT) - We explored innovative approaches to Data Operations and new concepts such as Data Mesh and translated these complex data concepts into practical suggestions applicable to the CMS data landscape and vision. This evolved into the formation of the Enterprise Data Lake. As the product manager, we created a product roadmap to lay out the tasks to bring the EDL to fruition. We have worked with multiple stakeholders across the enterprise to promote the adoption of EDL. For instance, we are currently working with OAGM to integrate CALM into the EDL. We also conceived of a new area focus entitled Customer Enablement to devise mechanisms to make it easy for customers to access data. We helped drive the EDL Governance through POCs, Concepts of Operations, the Enterprise Data Lake Confluence site, presentations and demos, and one-on-one conversations with stakeholders.\\n\\nMDM Redesign (OIT) - Our team developed potential future architecture options for the MDM Redesign. We reviewed potential vendor products and coordinated POCs with those products to ensure that the capabilities stakeholders valued most would be met. We saw that the proposed solution was expensive, overloaded with unneeded features, and weak in the most critical area: identity resolution.  We suggested using cloud-based alternatives that were of a lighter footprint and potentially lower cost to CMS. CMS saved money by selecting a capable best-fit product that achieves the functionality required by CMS.'),\n",
       "  Document(metadata={'source': 'rag_data/nimbus/CS_Nimbus_Consulting_Capability_Statement_for_Strategic_Enterprise_and_Alignment_SEA_and_Program_Management_PM_Support.docx'}, page_content='Data Governance (OIT) - We have provided expert consultation to CMS for the planning and implementation of an Enterprise-wide Data Governance program.\\n\\nEnterprise Data Lake (EDL) (OIT) - We explored innovative approaches to Data Operations and new concepts such as Data Mesh and translated these complex data concepts into practical suggestions applicable to the CMS data landscape and vision. This evolved into the formation of the Enterprise Data Lake. As the product manager, we created a product roadmap to lay out the tasks to bring the EDL to fruition. We have worked with multiple stakeholders across the enterprise to promote the adoption of EDL. For instance, we are currently working with OAGM to integrate CALM into the EDL. We also conceived of a new area focus entitled Customer Enablement to devise mechanisms to make it easy for customers to access data. We helped drive the EDL Governance through POCs, Concepts of Operations, the Enterprise Data Lake Confluence site, presentations and demos, and one-on-one conversations with stakeholders.\\n\\nMDM Redesign (OIT) - Our team developed potential future architecture options for the MDM Redesign. We reviewed potential vendor products and coordinated POCs with those products to ensure that the capabilities stakeholders valued most would be met. We saw that the proposed solution was expensive, overloaded with unneeded features, and weak in the most critical area: identity resolution.  We suggested using cloud-based alternatives that were of a lighter footprint and potentially lower cost to CMS. CMS saved money by selecting a capable best-fit product that achieves the functionality required by CMS.'),\n",
       "  Document(metadata={'source': 'rag_data/nimbus/Relevant_Experience_Project_1.docx'}, page_content='Work Stream – 15– Support for DDES in reviewing Master Data Management Activities.\\n\\nThe FET shall provide subject matter expertise, data analytics, engineering and architectural design expertise in support of the DDES effort to guide and direct the MDM team.  The MDM team is integrating Provider and Beneficiary data from both Medicare and Medicaid and exploring data in ways new to CMS.  Data analysis is required to ensure the quality of the integration processes and data products.  In addition, data analytics are needed to identify data issues spanning all the source Provider data systems feeding MDM’s curated data products. The FET shall perform tasks as requested by CMS and report results directly to CMS. These tasks include, but are not limited to:\\n\\n• Analyze MDM data and perform UAT\\n• Support EADG and DDES reviews of MDM-recommended products and solutions as well as data and architectural approaches\\n• Perform quality control and analyze MDM data to understand data challenges and suggest practical solutions\\n\\nWork Stream – 16– Customer Empowerment and Enablement.'),\n",
       "  Document(metadata={'source': 'rag_data/nimbus/Relevant_Experience_Project_1.docx'}, page_content='Work Stream – 7 – Optimizing Current Platforms and Understanding Emerging Technology\\n\\nThis on-going effort requires expert understanding of the CMS infrastructure, data centers, platforms, and services.\\xa0 In addition, the FET shall monitor industry technical directions and advise CMS per opportunities that may present themselves in a rapidly changing industry.\\xa0 Staff to provide support for POCs for virtualization of data such as Graph Analysis.\\xa0 The FET must help CMS increase our understanding of Master Data Management identity resolution technologies in the cloud.\\n\\nWork Stream 8 - MDM Provider Data Analytics  \\n\\nThe MDM team is integrating Provider data in ways new to CMS.\\xa0 Data analysis is required to ensure the quality of the integration processes and data products.\\xa0 In addition, data analytics are needed to identify data issues spanning all the source Provider data systems feeding MDM – through a series of comparisons of one data set to another.\\xa0\\xa0 Where appropriate, the Provider data analysis findings are summarized on a monthly basis for consideration by CMS business and technical leads.\\xa0\\n\\nWork Stream – 9 – MDM 2.0 Support\\n\\nThe MDM system footprint is undergoing a major upgrade (V 11.5) to the underlying MDM integration engine software package.\\xa0\\xa0 Complementing the MDM software product upgrade, the overall data ingest and storage approach changes significantly.\\xa0\\xa0 The team will assist CMS coordinate and support this major redesign and upgrade, known as ‘MDM 2.0’, while maintaining existing products and service levels\\n\\nWork Stream – 10– Portfolio Data Storage and Services Engineering and Analysis Support')],\n",
       " 'answer': 'Sure, I can expand the data quality section to cover more capabilities in that area:\\n\\nData Quality\\n• Developing comprehensive data quality management strategies and roadmaps\\n• Defining data quality rules, metrics, thresholds and service level agreements (SLAs)\\n• Performing data profiling to analyze completeness, accuracy, consistency, duplication, integrity\\n• Designing and implementing data cleansing, standardization, and de-duplication processes\\n• Establishing data quality scorecards, dashboards and monitoring for continuous measurement\\n• Expertise in parsing, matching, survivorship, and master data management (MDM) \\n• Implementing data quality firewalls and real-time data validation checks\\n• Experience with data quality tools like Informatica, SAS Data Quality, Talend, Trillium, etc.\\n• Skilled in SQL, Python, R and other programming for customized data quality routines\\n• Defining data quality certification processes for vetting new data sources\\n• Conducting root cause analysis of data quality issues and remediation planning\\n• Developing data quality training programs and knowledge transfer\\n\\nThis expanded section covers key data quality capabilities like:\\n- Data profiling and analysis \\n- Designing cleansing, standardization, de-duplication processes\\n- Metrics, scorecards, monitoring and continuous measurement  \\n- Data quality firewalls and validation rules\\n- Data quality tools expertise\\n- Custom data quality programming\\n- Data certification processes  \\n- Root cause analysis and remediation\\n- Data quality training\\n\\nLet me know if you would like me to elaborate on any part of the data quality services in more detail.'}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"The data quality section is very narrow, could you please expand on that a bit more? \"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "follow_up_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a1128b-f08d-4f4c-a8f7-e256c54be75a",
   "metadata": {},
   "source": [
    "#### Now ask a random question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d7198c-e8f7-4141-9cdd-a1084b68d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "follow_up_result = chain_with_history.invoke(\n",
    "    {\"input\": \"Give me a few tips on how to plant a  new garden.\"},\n",
    "    config={\"configurable\": {\"session_id\": \"session_1\"}}\n",
    ")\n",
    "follow_up_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1e17fc-b148-4989-819f-e19b60bbc140",
   "metadata": {},
   "source": [
    "Let's see how the semantic search works:\n",
    "1. First we calculate the embeddings vector for the query, and\n",
    "2. then we use this vector to do a similarity search on the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee013f8-62d4-4070-a898-62dfaa7a8f18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141fefcd-0135-4bf3-b0f1-d14b0faeaaa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
